{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing DataScience libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm,skew\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression,Ridge,RidgeCV, ElasticNetCV, LassoCV,BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40093660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load train_dataset.csv and test_dataset.csv\n",
    "train_dataset = pd.read_csv('train_dataset.csv')\n",
    "test_dataset = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da589aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dadb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The rows and columns of our dataset \n",
    "train_dataset.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78245683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well we have to deal with plenty of attributes \n",
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype of each attribute\n",
    "train_dataset.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003fa089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics of Numerical Variables\n",
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c68976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Statistics of our Categorical variables\n",
    "train_dataset.describe(include=['O'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the Missing values\n",
    "# Using isnull fuction to count the total null values in each field\n",
    "total = train_dataset.isnull().sum().sort_values(ascending=False) \n",
    "# Percent of missing values is estimated by dividing total missing and the original total\n",
    "percent = (train_dataset.isnull().sum()/train_dataset.isnull().count()).sort_values(ascending=False)\n",
    "# Concatenating the Total and Percent fields sing pandas concat fucntion\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# Displays top 20 from our max sorted list\n",
    "missing_data.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well! Things look better now! \n",
    "train_dataset.isnull().sum().max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f025d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dataset['headline'], train_dataset['clickbait'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the classification model\n",
    "classification_model = RandomForestClassifier(random_state=42)\n",
    "classification_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classification_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classification model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf79fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset (assuming 'test_data.csv' as the file name)\n",
    "test_dataset = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test_tfidf = vectorizer.transform(test_dataset['headline'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = classification_model.predict_proba(X_test_tfidf)[:, 1]  # Probability of being clickbait\n",
    "\n",
    "# Assuming you have already obtained test_predictions from your model\n",
    "\n",
    "threshold = 0.5  # Set your threshold value here\n",
    "\n",
    "# Convert probabilities to binary predictions based on the threshold\n",
    "binary_predictions = [1 if prob >= threshold else 0 for prob in test_predictions]\n",
    "\n",
    "# Create a DataFrame with ID and predicted probabilities\n",
    "submission_df = pd.DataFrame({'ID': test_dataset['ID'], 'clickbait': binary_predictions})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55020bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.clickbait(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8a47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca876b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fd09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600a46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02bce13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fba4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef9faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321fe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42366c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a4955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
